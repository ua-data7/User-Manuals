---
title: "UA HPC"
format:
  html:
    theme: cosmo
    code-copy: true
    toc: true
---

Intoduction to HPC

UArizona HPC systems are funded through the Office of Research Discovery and Innovation (ORDI), Chief Information Officer (CIO), and University Information Technology Services (UITS).

All UArizona HPC systems are available to all UArizona researchers (faculty, staff, undergrad and grad students, postdocs and DCC's) at no cost. Each research group is provided 7,000 CPU-hours per month on ElGato, 70,000 CPU-hours per month on Ocelote (increased from 35,000 in November 2022), and an astonishing 100,000 CPU-hours per month on Puma (increased from 70,000 hours in April 2022).

Access to these systems is through a linux command line interface which may be unfamiliar to the new user.  This section has the basic knowledge that will introduce you to the resources and provides information on account registration, system access, how to run jobs, and how to request help.

Getting Started and Training

About Super Computers

A supercomputer is a collection, or cluster, of a large number of regular computers (referred to as compute nodes) connected over a network. Each of the computers is like a local workstation though typically much more capable. For example, a standard laptop might have 4 CPUs and 8gb of RAM. Compare this with a standard compute node on Puma which has a whopping 94 CPUs and 470gb of RAM!

Another thing that differentiates supercomputers from your personal workstation is a supercomputer is a shared resource. This means there may be hundreds or even thousands of simultaneous users. Without some sort of coordination, you can imagine it would be a logistical nightmare to figure out who can run their code, what resources they can use, and where they should run it. That's why supercomputers use login nodes and job schedulers (we use one called SLURM).

HPC Supercomputer Overview

1. Puma

Puma is our latest supercomputer which came online in the middle of 2020. As is the case for our other supercomputers, we use the RFP process to get the best value for our financial resources, that meet our technical requirements.  This time Penguin Computing one with AMD processors. To understand the specifications, learn about the features of here.

2. Ocelote

Ocelote arrived in 2016.  Lenovo's Nextscale M5 technology was the winner of the RFP mainly on price, performance and meeting our specific requirements.

In 2021, Ocelote's operating system was upgraded from CentOS6 to CentOS7 and was configured to use SLURM, like Puma. It will continue until it is either too expensive to maintain or it is replaced by something else. To understand the specifications, learn about the features of here.

3. ElGato

ElGato is the cluster we obtained prior to Ocelote and was rebuilt last year with CentOS 7. As of July 2021, ElGato was upgraded to use Slurm.

Puma Quick Start Guide

Follow this link to gain an overview of how to run your work on our systems. By the end of this, you should know:

What HPC is

How to log in

What the bastion host, login nodes, and compute nodes are

What a job scheduler is 

How to access software

How to run interactive and batch jobs on HPC

If you have not already, you will need to Register for an HPC Account to follow along.

Account Creation

Who is Eligible for an HPC Account?

All UArizona Faculty, Staff, Students, and Affiliates are eligible for HPC accounts.

Faculty members will receive a PI Account which is different from the Sponsored Accounts that staff, students, and affiliates receive. PI Accounts are self-sponsored, manage their own HPC group(s), and receive their own time and storage allocations to be shared among group members. Faculty members may add other users to their groups to grant them HPC access. Users may be a member of more than one HPC group.

Detailed user guide for account creation is available here. 

System Access

Logging into the HPC supercomputers starts with your UArizona NetID and password with two-factor authentication enabled. Logging in will first connect you to something called the bastion host, a computer that provides a gateway to our three clusters: Ocelote, ElGato, and Puma. This is the only function the bastion host serves. It is not for storing files, running programs, or accessing software. A comprehensive walkthrough of this process is in our Puma Quick Start page. Follow the link to provide you with instructions on getting terminal access to the system from your specific OS, how to log into the system from our web interface (Open OnDemand), how to set up X11 (image) forwarding, and how to configure your account to allow for a password-less login (see: SSH Keys).

Training and Workshops

Overview

YouTube Videos | Workshops and Schedule | Machine Learning in Python | Machine Learning in R | External Resources Training | Self Guided Training | Data Center Video Tour

HPC Workshops

Every semester, we offer a variety of workshops including, but not limited to, Intro to HPC, Intro to Machine Learning, Intro to Parallel Computing, Intro to Containers, and Data Management Workshops. Check the Workshops and Schedule section to see the dates of our upcoming sessions or check out the links on the right-hand side for detailed information. We announce upcoming workshops through the hpc-announce listserv so if you do not see any workshops scheduled, keep your eye on your inbox. 

External Training Resources

Other organizations provide great training opportunities. Check out this list for upcoming events.

Self Guided Training

Need some help getting started with Linux, GPU programming, Singularity, OpenMP, or Matlab? Check out the Self Guided Training section for resources to get you up and running. 

Best Practices

You share Ocelote, ElGato, and Puma with many other users and what you do can affect others. Exercise good citizenship to ensure that your activity does not adversely impact the system and the research community with whom you share it. 

Visit this page to learn some rules of thumb.

Data Storage and Transfer

Storage

Data undergoing active analyses should be stored in HPC's local High Performance Storage (Tier 1).

 Large amounts of data that could be considered "warm" can be stored at reasonable rates on our Rental Storage.

Research data not requiring immediate access should be stored in General Research Data Storage (Tier 2). For example:

Large datasets where only subsets are actively being analyzed

Results no longer requiring immediate access

Backups (highly encouraged!)

Data no longer involved in ongoing analyses that need long-term archiving should be stored in Long term Research Storage (Tier 3).

Transferring Data

The bastion host provides secure access to the HPC supercomputers, has limited storage capacity and is not intended for file transfers.

To make transfers to/from HPC, you will need to have logged into your account at least once. If you have not, you may encounter "directory does not exist" errors. This is because your home directory is not created until you log in for the first time. See here about System Access

Files are transferred to shared data storage and not to the bastion node, login nodes, or compute nodes. Because the storage is shared, your files are accessible on all clusters; Puma, Ocelote and Elgato. 

Data Transfers by Size

Small Transfers: For small data transfers the web portal offers the most intuitive method.

Transfers <100GB: we recommend sftp, scp or rsync using filexfer.hpc.arizona.edu.  

Transfers (>100GB), transfers outside the university, and large transfers within HPC: we recommend using Globus (GridFTP).

To learn about Transfer Software Summary, SSH Keys, Endpoints, SFTP, FTP/LFTP, SCP, rsync, iRODS, rclone with examples, follow this link.

Software and Resources

Accessing Software

The policies regarding the installation of software are on this page.  In general, scientific software is installed as requested with the caveats noted in that section. There are over 100 software applications installed as modules so you should look there before submitting an installation request. As an alternative, you are always welcome to install your own software or other software in your file space.

Puma, Ocelote, and ElGato are built with CentOS 7 along with the system libraries, compilers and utilities that are needed for HPC operations. 

Workflow Managers

Supporting data-centric science involves the movement of data, multi-stage processing, and visualization at scales where manual control becomes prohibitive and automation is needed. Workflow technologies can improve the productivity and efficiency of data-centric science by orchestrating and automating these steps.

Visualization

Overview:Visualization Consulting| Software (Paraview | Blender | Visit) | GUIs Through Open OnDemand | VPN - Virtual Private Network | X11 Examples (Ansys Workbench | GLX - Useful for VisIt)

GPU Nodes

Compute Resources

More detailed information on system resources can be found on our Compute Resources page.

Containers with GPU Support

Singularity containers are available as modules on HPC for GPU-supported workflows. For more information, see our documentation on Containers.

Accessing GPUs

Information on how to request GPUs using SLURM can be found in our SLURM Documentation.

Training

For a list of training resources related to GPU workflows, see our Training documentation.

Running Jobs

Running Jobs with SLURM

All three clusters, Puma, Ocelote, and ElGato, use SLURM for resource management and job scheduling. 

Job Examples

When submitting jobs with named output files (i.e. with the line #SBATCH -o=Job.out) as arrays, SLURM will write every array element to that filename leaving you with only the output of the last completed job in the array. Use one of the following SLURM directives in your script to prevent this behavior.

For more examples, visit https://ua-researchcomputing-hpc.github.io/

Open On Demand

Open OnDemand, which is an NSF-funded open-source HPC portal, is available for users and provides web browser access for interfacing with HPC. This service is available from https://ood.hpc.arizona.edu/

Accounting, Limits, and Groups

Research and Class Groups

HPC groups are ways for faculty members to manage file permissions, job allocations, and group members. There are two types: Research and Class groups.

Research groups include any faculty, postdocs, graduate students, DCCs, staff, or student workers actively affiliated with your group's research. 

Class groups are for educational purposes only and will include students enrolled in a semester-long course.

Allocation and Limits

Storage Allocations

When you obtain a new HPC account, you will be provided with storage.  The shared storage (/home, /groups, /xdisk) is accessible from any of the three production clusters: Puma, Ocelote and ElGato.

Job Allocations

All University of Arizona Principal Investigators (PIs; aka Faculty) that register for access to the UA High Performance Computing (HPC) receive these free allocations on the HPC machines which is shared among all members of their team.

Job Limits

To check group, user, and job limitations on resource usage, use the command job-limits $YOUR_GROUP in the terminal.

Getting Help

Support Services

If you need any help or would like to learn more about our Consulting Services and support policies, please visit the following link:https://public.confluence.arizona.edu/display/UAHPC/Getting+Help.

FAQs

How do I access a supercomputer?

Though you're physically distant from the UArizona supercomputers, you can access them directly from the comfort of your own local workstation. This can either be done from a local terminal (for Windows users, using something like PuTTY) or by using our web interface. To begin the process, you'll first need to request an account at https://public.confluence.arizona.edu/display/UAHPC/Account+Creation.

Are there any resources available to help me get started?

Absolutely. We have a quick start tutorial (https://public.confluence.arizona.edu/display/UAHPC/Puma+Quick+Start) that can help walk you through the specifics of logging in and submitting a job for the first time. We have an Intro to HPC workshop video recording (https://public.confluence.arizona.edu/display/UAHPC/Training#Training-IntroductiontoHPC) that goes over the basics of supercomputers and how to use them. We also have consultants available to provide support. They can be reached either via a ServiceNow ticket or by dropping by our virtual office hours held every Wednesday from 2:00-4:00pm.

Are any software modules loaded by default?

Yes, when you start an interactive terminal session or submit a batch script, the modules ohpc, gnu8, openmpi3, and cmake are automatically loaded. If your code uses Intel compilers, you will want to manually unload gnu8 and openmpi3 to prevent conflicts. The exception: If you are working in a terminal in an Open OnDemand interactive desktop session, nothing is loaded by default and you will need to manually load any necessary modules.

For more FAQs regarding Account Access, General Computing, Jobs and Scheduling, Software, Data Storage/ Transfer and other Secure Services, visit https://public.confluence.arizona.edu/display/UAHPC/FAQ.
